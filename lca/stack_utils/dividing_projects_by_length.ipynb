{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cfabe8d",
   "metadata": {},
   "source": [
    "Cells in this notebook are responsible to perform all the operations to divide your dataset into a number of parquet files by the number of lines in a whole project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4adb8c16",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import time\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01866e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e083f60",
   "metadata": {},
   "source": [
    "### Creating a dictionary to map repository names to `int`\n",
    "\n",
    " - `parquet_path` -- path to the initial dataset\n",
    " - `repo_name_column` -- column with the repository names \n",
    " - `repo_to_id_filename` -- name of json file with the mapping, will be saved in `data_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f80ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = '/mnt/data/glukhov/big_context_python_starcoder_tokenized.parquet'\n",
    "\n",
    "df = dd.read_parquet(parquet_path, engine='pyarrow')\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc99a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name_column = 'max_stars_repo_name'\n",
    "\n",
    "df = df[repo_name_column].unique().compute()\n",
    "names_list = df.to_list()\n",
    "\n",
    "repo_name_to_id = {name: idx for idx, name in enumerate(names_list)}\n",
    "\n",
    "print('Number of unique repo names: ', len(repo_name_to_id.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a3c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_to_id_filename = 'repo_to_id.json'\n",
    "\n",
    "with open(os.path.join(data_dir, repo_to_id_filename), 'w') as f:\n",
    "    json.dump(repo_name_to_id, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4d7f1",
   "metadata": {},
   "source": [
    "### Calculating project lenghts (in lines)\n",
    "\n",
    " - `repo_to_id_filename` -- name of json file with the repo_names -> int mapping, should be in `data_dir`\n",
    " - `parquet_path` -- path to the initial dataset\n",
    " - `repo_name_column` -- column with the repository names\n",
    " - `content_column` -- column with the content (`str`)\n",
    " - `lines_count_filename` -- parquet file with two `int` columns: `repo_id`, `content_length`, will be saved to `data_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228623bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lines(content: str, newline_caracter='\\n') -> int:\n",
    "    return content.count(newline_caracter)\n",
    "\n",
    "def repo_to_id(reponame: str, repo_to_id_dict: dict[str, int]) -> int:\n",
    "    return repo_to_id_dict[reponame]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_to_id_filename = 'repo_to_id.json'\n",
    "\n",
    "with open(os.path.join(data_dir, repo_to_id_filename), 'r') as f:\n",
    "    repo_to_id_dict = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd589c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = '/mnt/data/glukhov/python_starcoder.parquet'\n",
    "\n",
    "df = dd.read_parquet(parquet_path, engine='pyarrow')\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea4453",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name_column = 'max_stars_repo_name'\n",
    "content_column = 'content'\n",
    "\n",
    "df = df[[repo_name_column, content_column]]\n",
    "\n",
    "df['repo_id'] = df[repo_name_column].apply(lambda x: repo_to_id(x, repo_to_id_dict), meta=('repo_id', int))\n",
    "df = df[['repo_id', content_column]]\n",
    "\n",
    "df['content_length'] = df[content_column].apply(lambda x: calculate_lines(x,), meta=('content_length', int))\n",
    "df = df[['repo_id', 'content_length']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e63e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_count_filename = 'lines_per_file.parquet'\n",
    "\n",
    "df.compute().to_parquet(os.path.join(data_dir, lines_count_filename), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e27d410",
   "metadata": {},
   "source": [
    "### Creating a repositories split (outliers and percentiles)\n",
    "\n",
    "Dataset wil be divided into `num_parts` parts (100%/`num_parts` of all projects without the outliers each) and one part with the outliers. Corresponding `repo_id`'s will be saved in csv-files.\n",
    "\n",
    " - `lines_count_filename` -- parquet file with two `int` columns: `repo_id`, `content_length`, should be in `data_dir`\n",
    " - `split_dir_name` -- name of directory inside `dir_name` with split files\n",
    " - `outlier_percentile` -- percentile for outliers\n",
    " - `num_parts` -- number of categories in the split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e0516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_count_filename = 'lines_per_file.parquet'\n",
    "\n",
    "df = pd.read_parquet(os.path.join(data_dir, lines_count_filename))  # change to dd.read_parquet if file is too big\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9761f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dir_name = 'repo_ids_by_length'\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, split_dir_name)):\n",
    "    os.mkdir(os.path.join(data_dir, split_dir_name))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8dd762",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_percentile = 0.995\n",
    "\n",
    "df_repo_len = df.groupby('repo_id')['content_length'].sum()\n",
    "outlier_value = df_repo_len.quantile(outlier_percentile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcd9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parts = 6\n",
    "\n",
    "split = pd.qcut(df_repo_len[df_repo_len<=outlier_value], q=num_parts).rename('bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in split.unique():\n",
    "    cat_df = split[split==category].reset_index()['repo_id']\n",
    "    filename = 'repo_ids_lines_' + f'{category}'[1:-1].replace('.0', '').replace(', ', '_') + '.csv'\n",
    "    cat_df.to_csv(os.path.join(data_dir, split_dir_name, filename), index=False)\n",
    "    \n",
    "df_largest = df_repo_len[df_repo_len>outlier_value].reset_index()['repo_id']\n",
    "filename = 'repo_ids_lines_' + f'{int(outlier_value)}_inf' + '.csv'\n",
    "df_largest.to_csv(os.path.join(data_dir, split_dir_name, filename), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f0f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_largest.shape, outlier_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17022a2a",
   "metadata": {},
   "source": [
    "### Dividing the dataset by the projects lengths\n",
    "\n",
    " - `repo_to_id_filename` -- name of json file with the repo_names -> int mapping, should be in `data_dir`\n",
    " - `parquet_path` -- path to the initial dataset\n",
    " - `repo_name_column` -- column with the repository names\n",
    " - `content_column` -- column with the content (`str`)\n",
    " - `filename_column`\n",
    " - `split_dir_name` -- name of directory inside `dir_name` with csv split files\n",
    " - `dataset_dir` -- name of directory inside `dir_name` with splitted dataset\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_to_id_filename = 'repo_to_id.json'\n",
    "\n",
    "with open(os.path.join(data_dir, repo_to_id_filename), 'r') as f:\n",
    "    repo_to_id_dict = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff315f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = '/mnt/data/glukhov/python_starcoder.parquet'\n",
    "\n",
    "df = dd.read_parquet(parquet_path, engine='pyarrow')\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name_column = 'max_stars_repo_name'\n",
    "content_column = 'content'\n",
    "filename_column = 'max_stars_repo_path'\n",
    "\n",
    "df = df[[repo_name_column, content_column, filename_column]]\n",
    "\n",
    "df['repo_id'] = df[repo_name_column].apply(lambda x: repo_to_id(x, repo_to_id_dict), meta=('repo_id', int))\n",
    "df = df[['repo_id', content_column, filename_column]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa238f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dir_name = 'repo_ids_by_length'\n",
    "dataset_dir = 'length_divided_dataset'\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, dataset_dir)):\n",
    "    os.mkdir(os.path.join(data_dir, dataset_dir))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5479197",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(os.listdir(os.path.join(data_dir, split_dir_name))):\n",
    "    repo_ids = pd.read_csv(os.path.join(data_dir, split_dir_name, filename))\n",
    "    curr_df = df.merge(repo_ids, on='repo_id', how='inner')\n",
    "    file_path = os.path.join(data_dir, dataset_dir, f'{filename.replace(\"repo_ids_lines\", \"num_lines\").replace(\".csv\",\"\")}.parquet')\n",
    "    curr_df.sort_values('repo_id').to_parquet(file_path, engine='pyarrow', write_index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "split.unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1f2c2a",
   "metadata": {},
   "source": [
    "To load the data use:\n",
    " 1. `pd.read_parquet('./data/length_divided_dataset/num_lines_278_538.parquet')` \n",
    " 1. `pd.read_parquet('./data/length_divided_dataset/num_lines_278_538.parquet/part.12.parquet')`\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74abdf2d",
   "metadata": {},
   "source": [
    "### Creating jsons with 2 mappings `file` <-> `repo_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970e2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_content = dict()\n",
    "for parquet_dir in tqdm(os.listdir(os.path.join(data_dir, dataset_dir))):\n",
    "    for parquet_file in tqdm(os.listdir(os.path.join(data_dir, dataset_dir, parquet_dir))):\n",
    "        filepath = os.path.join(data_dir, dataset_dir, parquet_dir, parquet_file)\n",
    "        df = pd.read_parquet(filepath)\n",
    "        parquet_content[os.path.join(parquet_dir, parquet_file)] = df['repo_id'].unique().tolist()\n",
    "\n",
    "with open(os.path.join(data_dir, 'file_to_repo_id.json'), 'w') as f:\n",
    "    json.dump(parquet_content, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa4b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'file_to_repo_id.json'), 'r') as f:\n",
    "    parquet_content = json.load(f)\n",
    "\n",
    "id_2_filename = dict()\n",
    "for filename, repo_ids in tqdm(parquet_content.items()):\n",
    "    for repo_id in repo_ids:\n",
    "        if repo_id not in id_2_filename:\n",
    "            id_2_filename[repo_id] = list()\n",
    "\n",
    "        id_2_filename[repo_id].append(filename)\n",
    "\n",
    "with open(os.path.join(data_dir, 'repo_id_to_file.json'), 'w') as f:\n",
    "    json.dump(id_2_filename, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abae62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average number of partition files for a project: ', sum(len(i) for i in id_2_filename.values()) / len(id_2_filename.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e05e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
