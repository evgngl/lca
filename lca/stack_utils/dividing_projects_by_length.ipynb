{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cfabe8d",
   "metadata": {},
   "source": [
    "Cells in this notebook are responsible to perform all the operations to divide your dataset into a number of parquet files by the number of lines in a whole project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4adb8c16",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import time\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01866e9a",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../../data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e083f60",
   "metadata": {},
   "source": [
    "### Creating a dictionary to map repository names to `int`\n",
    "\n",
    " - `parquet_path` -- path to the initial dataset\n",
    " - `repo_name_column` -- column with the repository names \n",
    " - `repo_to_id_filename` -- name of json file with the mapping, will be saved in `data_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "812f80ef",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         max_stars_repo_name   \n",
      "0                               crempp/mdweb  \\\n",
      "1                       lrq3000/markdown2zim   \n",
      "2                            MrOlm/glrestore   \n",
      "3                           bouvierj/chimbot   \n",
      "4  issa-project/entity-fishing-client-python   \n",
      "\n",
      "                                      joined_content  \n",
      "0  tests/test_debug_helper.pyêåº# -*- coding: utf-8...  \n",
      "1  markdown2zim.pyêåº#!/usr/bin/python2\\n# -*- codi...  \n",
      "2  glrestore/glrestore.pyêåº<reponame>MrOlm/glresto...  \n",
      "3  plugins/starter.pyêåºimport time\\nimport re\\nimp...  \n",
      "4  nerd/nerd_client.pyêåº<reponame>issa-project/ent...  \n"
     ]
    }
   ],
   "source": [
    "parquet_path = '/mnt/data/glukhov/big_context_python_starcoder.parquet'\n",
    "\n",
    "df = dd.read_parquet(parquet_path, engine='pyarrow')\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ccc99a3",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique repo names:  1678572\n"
     ]
    }
   ],
   "source": [
    "repo_name_column = 'max_stars_repo_name'\n",
    "\n",
    "df = df[repo_name_column].unique().compute()\n",
    "names_list = df.to_list()\n",
    "\n",
    "repo_name_to_id = {name: idx for idx, name in enumerate(names_list)}\n",
    "\n",
    "print('Number of unique repo names: ', len(repo_name_to_id.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a3c7b4",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "repo_to_id_filename = 'repo_to_id.json'\n",
    "\n",
    "with open(os.path.join(data_dir, repo_to_id_filename), 'w') as f:\n",
    "    json.dump(repo_name_to_id, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4d7f1",
   "metadata": {},
   "source": [
    "### Calculating project lenghts (in lines)\n",
    "\n",
    " - `repo_to_id_filename` -- name of json file with the repo_names -> int mapping, should be in `data_dir`\n",
    " - `parquet_path` -- path to the initial dataset\n",
    " - `repo_name_column` -- column with the repository names\n",
    " - `content_column` -- column with the content (`str`)\n",
    " - `lines_count_filename` -- parquet file with two `int` columns: `repo_id`, `content_length`, will be saved to `data_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "228623bf",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def calculate_lines(content: str, newline_caracter='\\n') -> int:\n",
    "    return content.count(newline_caracter)\n",
    "\n",
    "def repo_to_id(reponame: str, repo_to_id_dict: dict[str, int]) -> int:\n",
    "    return repo_to_id_dict[reponame]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d472d5a3",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "repo_to_id_filename = 'repo_to_id.json'\n",
    "\n",
    "with open(os.path.join(data_dir, repo_to_id_filename), 'r') as f:\n",
    "    repo_to_id_dict = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd589c71",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 max_stars_repo_path   \n",
      "0                         public_data/serializers.py  \\\n",
      "1                              quick_search/admin.py   \n",
      "2                                      rasa/train.py   \n",
      "3  coding_intereview/1475. Final Prices With a Sp...   \n",
      "4               rplugin/python3/denite/ui/default.py   \n",
      "\n",
      "                max_stars_repo_name id   \n",
      "0                   MTES-MCT/sparte  0  \\\n",
      "1     naman1901/django-quick-search  1   \n",
      "2  Amirali-Shirkh/rasa-for-botfront  2   \n",
      "3        Jahidul007/Python-Bootcamp  3   \n",
      "4            timgates42/denite.nvim  4   \n",
      "\n",
      "                                             content  \n",
      "0  <reponame>MTES-MCT/sparte\\nfrom rest_framework...  \n",
      "1  from django.contrib import admin\\nfrom .models...  \n",
      "2  import asyncio\\nimport os\\nimport tempfile\\nfr...  \n",
      "3  <gh_stars>1-10\\nclass Solution:\\n    def final...  \n",
      "4  <gh_stars>0\\n# ===============================...  \n"
     ]
    }
   ],
   "source": [
    "parquet_path = '/mnt/data/glukhov/python_starcoder.parquet'\n",
    "\n",
    "df = dd.read_parquet(parquet_path, engine='pyarrow')\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaea4453",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "repo_name_column = 'max_stars_repo_name'\n",
    "content_column = 'content'\n",
    "\n",
    "df = df[[repo_name_column, content_column]]\n",
    "\n",
    "df['repo_id'] = df[repo_name_column].apply(lambda x: repo_to_id(x, repo_to_id_dict), meta=('repo_id', int))\n",
    "df = df[['repo_id', content_column]]\n",
    "\n",
    "df['content_length'] = df[content_column].apply(lambda x: calculate_lines(x,), meta=('content_length', int))\n",
    "df = df[['repo_id', 'content_length']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0e63e83",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "lines_count_filename = 'lines_per_file.parquet'\n",
    "\n",
    "df.compute().to_parquet(os.path.join(data_dir, lines_count_filename), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e27d410",
   "metadata": {},
   "source": [
    "### Creating a repositories split (outliers and percentiles)\n",
    "\n",
    "Dataset wil be divided into `num_parts` parts (100%/`num_parts` of all projects without the outliers each) and one part with the outliers. Corresponding `repo_id`'s will be saved in csv-files.\n",
    "\n",
    " - `lines_count_filename` -- parquet file with two `int` columns: `repo_id`, `content_length`, should be in `data_dir`\n",
    " - `split_dir_name` -- name of directory inside `dir_name` with split files\n",
    " - `outlier_percentile` -- percentile for outliers\n",
    " - `num_parts` -- number of categories in the split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87e0516f",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "lines_count_filename = 'lines_per_file.parquet'\n",
    "\n",
    "df = pd.read_parquet(os.path.join(data_dir, lines_count_filename))  # change to dd.read_parquet if file is too big\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a9761f9",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "split_dir_name = 'repo_ids_by_length'\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, split_dir_name)):\n",
    "    os.mkdir(os.path.join(data_dir, split_dir_name))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f8dd762",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "outlier_percentile = 0.995\n",
    "\n",
    "df_repo_len = df.groupby('repo_id')['content_length'].sum()\n",
    "outlier_value = df_repo_len.quantile(outlier_percentile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fcd9101",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "num_parts = 6\n",
    "\n",
    "split = pd.qcut(df_repo_len[df_repo_len<=outlier_value], q=num_parts).rename('bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc47d119",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "for category in split.unique():\n",
    "    cat_df = split[split==category].reset_index()['repo_id']\n",
    "    filename = 'repo_ids_lines_' + f'{category}'[1:-1].replace('.0', '').replace(', ', '_') + '.csv'\n",
    "    cat_df.to_csv(os.path.join(data_dir, split_dir_name, filename), index=False)\n",
    "    \n",
    "df_largest = df_repo_len[df_repo_len>outlier_value].reset_index()['repo_id']\n",
    "filename = 'repo_ids_lines_' + f'{int(outlier_value)}_inf' + '.csv'\n",
    "df_largest.to_csv(os.path.join(data_dir, split_dir_name, filename), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b37f0f1c",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8393,) 16769.14500000002\n"
     ]
    }
   ],
   "source": [
    "print(df_largest.shape, outlier_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17022a2a",
   "metadata": {},
   "source": [
    "### Dividing the dataset by the projects lengths\n",
    "\n",
    " - `repo_to_id_filename` -- name of json file with the repo_names -> int mapping, should be in `data_dir`\n",
    " - `parquet_path` -- path to the initial dataset\n",
    " - `repo_name_column` -- column with the repository names\n",
    " - `content_column` -- column with the content (`str`)\n",
    " - `filename_column`\n",
    " - `split_dir_name` -- name of directory inside `dir_name` with csv split files\n",
    " - `dataset_dir` -- name of directory inside `dir_name` with splitted dataset\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c749a06f",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "repo_to_id_filename = 'repo_to_id.json'\n",
    "\n",
    "with open(os.path.join(data_dir, repo_to_id_filename), 'r') as f:\n",
    "    repo_to_id_dict = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ff315f8",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 max_stars_repo_path   \n",
      "0                         public_data/serializers.py  \\\n",
      "1                              quick_search/admin.py   \n",
      "2                                      rasa/train.py   \n",
      "3  coding_intereview/1475. Final Prices With a Sp...   \n",
      "4               rplugin/python3/denite/ui/default.py   \n",
      "\n",
      "                max_stars_repo_name id   \n",
      "0                   MTES-MCT/sparte  0  \\\n",
      "1     naman1901/django-quick-search  1   \n",
      "2  Amirali-Shirkh/rasa-for-botfront  2   \n",
      "3        Jahidul007/Python-Bootcamp  3   \n",
      "4            timgates42/denite.nvim  4   \n",
      "\n",
      "                                             content  \n",
      "0  <reponame>MTES-MCT/sparte\\nfrom rest_framework...  \n",
      "1  from django.contrib import admin\\nfrom .models...  \n",
      "2  import asyncio\\nimport os\\nimport tempfile\\nfr...  \n",
      "3  <gh_stars>1-10\\nclass Solution:\\n    def final...  \n",
      "4  <gh_stars>0\\n# ===============================...  \n"
     ]
    }
   ],
   "source": [
    "parquet_path = '/mnt/data/glukhov/python_starcoder.parquet'\n",
    "\n",
    "df = dd.read_parquet(parquet_path, engine='pyarrow')\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ba4a03e",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "repo_name_column = 'max_stars_repo_name'\n",
    "content_column = 'content'\n",
    "filename_column = 'max_stars_repo_path'\n",
    "\n",
    "df = df[[repo_name_column, content_column, filename_column]]\n",
    "\n",
    "df['repo_id'] = df[repo_name_column].apply(lambda x: repo_to_id(x, repo_to_id_dict), meta=('repo_id', int))\n",
    "df = df[['repo_id', content_column, filename_column]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7aa238f0",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "split_dir_name = 'repo_ids_by_length'\n",
    "dataset_dir = 'length_divided_dataset'\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, dataset_dir)):\n",
    "    os.mkdir(os.path.join(data_dir, dataset_dir))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5479197",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f41e6eb8ec48be96155c54defb1dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for filename in tqdm(os.listdir(os.path.join(data_dir, split_dir_name))):\n",
    "    repo_ids = pd.read_csv(os.path.join(data_dir, split_dir_name, filename))\n",
    "    curr_df = df.merge(repo_ids, on='repo_id', how='inner')\n",
    "    file_path = os.path.join(data_dir, dataset_dir, f'{filename.replace(\"repo_ids_lines\", \"num_lines\").replace(\".csv\",\"\")}.parquet')\n",
    "    curr_df.sort_values('repo_id').to_parquet(file_path, engine='pyarrow', write_index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e1b56",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "split.unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1f2c2a",
   "metadata": {},
   "source": [
    "To load the data use:\n",
    " 1. `pd.read_parquet('./data/length_divided_dataset/num_lines_278_538.parquet')` \n",
    " 1. `pd.read_parquet('./data/length_divided_dataset/num_lines_278_538.parquet/part.12.parquet')`\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74abdf2d",
   "metadata": {},
   "source": [
    "### Creating jsons with 2 mappings `file` <-> `repo_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970e2699",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "parquet_content = dict()\n",
    "for parquet_dir in tqdm(os.listdir(os.path.join(data_dir, dataset_dir))):\n",
    "    for parquet_file in tqdm(os.listdir(os.path.join(data_dir, dataset_dir, parquet_dir))):\n",
    "        filepath = os.path.join(data_dir, dataset_dir, parquet_dir, parquet_file)\n",
    "        df = pd.read_parquet(filepath)\n",
    "        parquet_content[os.path.join(parquet_dir, parquet_file)] = df['repo_id'].unique().tolist()\n",
    "\n",
    "with open(os.path.join(data_dir, 'file_to_repo_id.json'), 'w') as f:\n",
    "    json.dump(parquet_content, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa4b38",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'file_to_repo_id.json'), 'r') as f:\n",
    "    parquet_content = json.load(f)\n",
    "\n",
    "id_2_filename = dict()\n",
    "for filename, repo_ids in tqdm(parquet_content.items()):\n",
    "    for repo_id in repo_ids:\n",
    "        if repo_id not in id_2_filename:\n",
    "            id_2_filename[repo_id] = list()\n",
    "\n",
    "        id_2_filename[repo_id].append(filename)\n",
    "\n",
    "with open(os.path.join(data_dir, 'repo_id_to_file.json'), 'w') as f:\n",
    "    json.dump(id_2_filename, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abae62e",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print('Average number of partition files for a project: ', sum(len(i) for i in id_2_filename.values()) / len(id_2_filename.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e05e0",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb6ea23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['repo_id_to_file.json',\n",
       " 'repo_to_id.json',\n",
       " 'lines_per_file.parquet',\n",
       " 'test_ids.json',\n",
       " 'length_divided_dataset',\n",
       " 'file_to_repo_id.json',\n",
       " 'python_stack_split.json',\n",
       " 'repo_ids_by_length']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1f39d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_to_id_filename = 'repo_to_id.json'\n",
    "\n",
    "with open(os.path.join(data_dir, repo_to_id_filename), 'r') as f:\n",
    "    repo_to_id_dict = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cbc494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_filename = 'python_stack_split.json'\n",
    "\n",
    "with open(os.path.join(data_dir, split_filename), 'r') as f:\n",
    "    split_dict = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e19f6b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(repo_to_id_dict.keys()) == set(split_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01f4adf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Split.VAL', 'Split.TRAIN', 'Split.TEST'}\n"
     ]
    }
   ],
   "source": [
    "print(set(split_dict.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82fb108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = list()\n",
    "\n",
    "for repo_name in split_dict.keys():\n",
    "    if 'test' in split_dict[repo_name].lower():\n",
    "        test_ids.append(repo_to_id_dict[repo_name])\n",
    "        \n",
    "with open(os.path.join(data_dir, 'test_ids.json'), 'w') as f:\n",
    "    json.dump(test_ids, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8654d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test repositories: 84162\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of test repositories: {len(test_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3c010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "214ff72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids_filename = 'test_ids.json'\n",
    "\n",
    "with open(os.path.join(data_dir, test_ids_filename), 'r') as f:\n",
    "    test_ids = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0da87089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For lines 16769 inf number of repositories is:  406 out of 8393\n",
      "For lines 278 538 number of repositories is:  13846 out of 278211\n",
      "For lines -001 64 number of repositories is:  14111 out of 281809\n",
      "For lines 1239 16769 number of repositories is:  13936 out of 278221\n",
      "For lines 145 278 number of repositories is:  13860 out of 276442\n",
      "For lines 64 145 number of repositories is:  13858 out of 277500\n",
      "For lines 538 1239 number of repositories is:  14145 out of 277996\n"
     ]
    }
   ],
   "source": [
    "split_dir_name = 'repo_ids_by_length'\n",
    "\n",
    "for filename in os.listdir(os.path.join(data_dir, split_dir_name)):\n",
    "    df = pd.read_csv(os.path.join(data_dir, split_dir_name, filename))\n",
    "    print('For', filename[9:-4].replace('_', ' '),  'number of repositories is: ', df.isin(test_ids).sum().iloc[0],\n",
    "          'out of', df.size\n",
    "         )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea96307",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_2_parquet_filename = 'repo_id_to_file.json'\n",
    "\n",
    "with open(os.path.join(data_dir, id_2_parquet_filename), 'r') as f:\n",
    "    id_2_parquet = json.load(f)\n",
    "\n",
    "\n",
    "parquet_2_id_filename = 'file_to_repo_id.json'\n",
    "\n",
    "with open(os.path.join(data_dir, parquet_2_id_filename), 'r') as f:\n",
    "    parquet_2_id = json.load(f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de237819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3eae7dfbed49b88f5cc39ae197396e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_lines_-001_64.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d759c8bc390e49d2845242ec2a832e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_lines_538_1239.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add0d6413b8b4b31bf4c6b666b6cbc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_lines_1239_16769.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6a4b156e7a4d2d84e3df833dacfb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_lines_145_278.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ad885df0fa41499d88cc35e9b15328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_lines_16769_inf.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f1ee7425274660ab7129a849bca742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_lines_278_538.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7e5daed2b04578ac626f68a0d6e2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_lines_64_145.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52ab570670a4264b2767e802b8f687e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>content</th>\n",
       "      <th>max_stars_repo_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1394803</td>\n",
       "      <td>import os\\nimport testinfra.utils.ansible_runn...</td>\n",
       "      <td>molecule/resources/tests/test_default.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1394891</td>\n",
       "      <td>&lt;reponame&gt;r-mittal/inception-machina&lt;filename&gt;...</td>\n",
       "      <td>handling.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1395035</td>\n",
       "      <td>&lt;filename&gt;HW0/ec2OnAWS/src/aws_ec2.py\\nimport ...</td>\n",
       "      <td>HW0/ec2OnAWS/src/aws_ec2.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1395095</td>\n",
       "      <td>&lt;filename&gt;sphinxnotes/mock/__init__.py\\n\"\"\"\\n ...</td>\n",
       "      <td>sphinxnotes/mock/__init__.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1395115</td>\n",
       "      <td>import synthtool as s\\nimport synthtool.gcp as...</td>\n",
       "      <td>synth.py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    repo_id                                            content   \n",
       "8   1394803  import os\\nimport testinfra.utils.ansible_runn...  \\\n",
       "20  1394891  <reponame>r-mittal/inception-machina<filename>...   \n",
       "46  1395035  <filename>HW0/ec2OnAWS/src/aws_ec2.py\\nimport ...   \n",
       "67  1395095  <filename>sphinxnotes/mock/__init__.py\\n\"\"\"\\n ...   \n",
       "70  1395115  import synthtool as s\\nimport synthtool.gcp as...   \n",
       "\n",
       "                         max_stars_repo_path  \n",
       "8   molecule/resources/tests/test_default.py  \n",
       "20                               handling.py  \n",
       "46               HW0/ec2OnAWS/src/aws_ec2.py  \n",
       "67              sphinxnotes/mock/__init__.py  \n",
       "70                                  synth.py  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = 'length_divided_dataset'\n",
    "\n",
    "df_list = list()\n",
    "\n",
    "for parquet_fn in tqdm(os.listdir(os.path.join(data_dir, dataset_dir))):\n",
    "    parquet_path = os.path.join(data_dir, dataset_dir, parquet_fn)\n",
    "    print(parquet_fn)\n",
    "    df_part_list = list()\n",
    "    for partition_fn in tqdm(os.listdir(parquet_path)):\n",
    "        \n",
    "        test_ids_in_file = list(set(parquet_2_id[f'{parquet_fn}/{partition_fn}']).intersection(test_ids))\n",
    "        partition_path = os.path.join(parquet_path, partition_fn)\n",
    "        df_part = pd.read_parquet(partition_path)\n",
    "        df_part = df_part[df_part.repo_id.isin(test_ids_in_file)]\n",
    "        df_part_list.append(df_part)\n",
    "    df_list.append(pd.concat(df_part_list))\n",
    "        \n",
    "df = pd.concat(df_list)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.sort_values(by='repo_id')\n",
    "\n",
    "df.to_parquet(os.path.join(data_dir, 'test_dataset.parquet'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09c038d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(os.path.join(data_dir, 'test_dataset.parquet'),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b2db224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2082649/2078880708.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_multiple_files['is_context'].loc[index_to_completion] = False\n"
     ]
    }
   ],
   "source": [
    "df['is_context'] = True\n",
    "\n",
    "number_of_files = df.groupby('repo_id')['is_context'].sum()\n",
    "repo_ids_multiple_files = number_of_files[number_of_files > 1].index.to_list()\n",
    "df_multiple_files = df[df['repo_id'].isin(repo_ids_multiple_files)]\n",
    "\n",
    "index_to_completion = df_multiple_files.groupby('repo_id').sample(1).index.to_list()\n",
    "df_multiple_files['is_context'].loc[index_to_completion] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ccf1ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of projects with the following number of files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "is_context\n",
       "1     26985\n",
       "2     12361\n",
       "3      7993\n",
       "4      5972\n",
       "5      4445\n",
       "6      3583\n",
       "7      2886\n",
       "8      2386\n",
       "9      1918\n",
       "10     1584\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of projects with the following number of files')\n",
    "(df.groupby('repo_id').agg(list).reset_index()['is_context'].apply(lambda x: len(x))).value_counts().head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e532b282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>is_context</th>\n",
       "      <th>content</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>False</td>\n",
       "      <td>[&lt;reponame&gt;onlyhavecans/mmPython\\nfrom datetim...</td>\n",
       "      <td>[mm/utils.py]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>True</td>\n",
       "      <td>[&lt;gh_stars&gt;0\\n#!/usr/bin/env python\\nfrom setu...</td>\n",
       "      <td>[setup.py, mm/session.py, mm/logger.py, mm/fif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "      <td>[&lt;gh_stars&gt;0\\n__author__ = 'maartenbreddels'\\n...</td>\n",
       "      <td>[packages/vaex-core/vaex/dataset_mmap.py]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>True</td>\n",
       "      <td>[&lt;reponame&gt;yohplala/vaex\\n__version_tuple__ = ...</td>\n",
       "      <td>[packages/vaex-core/vaex/core/_version.py, tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77</td>\n",
       "      <td>False</td>\n",
       "      <td>[import argparse\\n\\nfrom bioplottemplates.libs...</td>\n",
       "      <td>[src/bioplottemplates/cli_labeldots.py]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   repo_id  is_context                                            content   \n",
       "0       36       False  [<reponame>onlyhavecans/mmPython\\nfrom datetim...  \\\n",
       "1       36        True  [<gh_stars>0\\n#!/usr/bin/env python\\nfrom setu...   \n",
       "2       60       False  [<gh_stars>0\\n__author__ = 'maartenbreddels'\\n...   \n",
       "3       60        True  [<reponame>yohplala/vaex\\n__version_tuple__ = ...   \n",
       "4       77       False  [import argparse\\n\\nfrom bioplottemplates.libs...   \n",
       "\n",
       "                                            filename  \n",
       "0                                      [mm/utils.py]  \n",
       "1  [setup.py, mm/session.py, mm/logger.py, mm/fif...  \n",
       "2          [packages/vaex-core/vaex/dataset_mmap.py]  \n",
       "3  [packages/vaex-core/vaex/core/_version.py, tes...  \n",
       "4            [src/bioplottemplates/cli_labeldots.py]  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_context_split = df_multiple_files.groupby(['repo_id', 'is_context']).agg(list).reset_index()\n",
    "df_context_split = df_context_split.rename(columns={'max_stars_repo_path': 'filename'})\n",
    "\n",
    "df_context_split.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d34e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For lines 16769 inf number of repositories is:  405 out of 8393\n",
      "For lines 278 538 number of repositories is:  11450 out of 278211\n",
      "For lines -001 64 number of repositories is:  3058 out of 281809\n",
      "For lines 1239 16769 number of repositories is:  13646 out of 278221\n",
      "For lines 145 278 number of repositories is:  9219 out of 276442\n",
      "For lines 64 145 number of repositories is:  6385 out of 277500\n",
      "For lines 538 1239 number of repositories is:  13014 out of 277996\n"
     ]
    }
   ],
   "source": [
    "split_dir_name = 'repo_ids_by_length'\n",
    "\n",
    "for filename in os.listdir(os.path.join(data_dir, split_dir_name)):\n",
    "    len_ids_df = pd.read_csv(os.path.join(data_dir, split_dir_name, filename))\n",
    "    print('For', filename[9:-4].replace('_', ' '),  'number of repositories is: ', len_ids_df.isin(repo_ids_multiple_files).sum().iloc[0],\n",
    "          'out of', len_ids_df.size\n",
    "         )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ebfd3a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce26ffd8a3145b2bcf714f93a7cb91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_to_repo_dict = {v: k for k, v in repo_to_id_dict.items()}\n",
    "\n",
    "assert df_context_split.shape[0] % 2 == 0\n",
    "df_context_split = df_context_split.sort_values(by=['repo_id', 'is_context'])\n",
    "\n",
    "benchmark_data = list()\n",
    "\n",
    "for i in tqdm(range(df_context_split.shape[0] // 2)):\n",
    "    datapoint = dict()\n",
    "    \n",
    "    row_completion = df_context_split.iloc[2*i]\n",
    "    row_context = df_context_split.iloc[2*i + 1]\n",
    "    assert row_completion['repo_id'] == row_context['repo_id']\n",
    "    \n",
    "    if row_completion['is_context']:\n",
    "        row_completion, row_context = row_context, row_completion\n",
    "    assert row_context['is_context'] and (not row_completion['is_context'])\n",
    "    assert int(row_context['repo_id']) == row_context['repo_id']\n",
    "    \n",
    "    datapoint['repo_id'] = int(row_context['repo_id'])\n",
    "    datapoint['repo_name'] = id_to_repo_dict[row_context['repo_id']]\n",
    "    \n",
    "    assert len(row_context['filename']) == len(row_context['content'])\n",
    "    context = dict(zip(row_context['filename'], row_context['content']))\n",
    "    datapoint['context'] = context\n",
    "    \n",
    "    assert len(row_completion['filename']) == len(row_completion['content'])\n",
    "    completion = dict(zip(row_completion['filename'], row_completion['content']))\n",
    "    datapoint['completion'] = completion\n",
    "    \n",
    "    benchmark_data.append(datapoint)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e403f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'benchmark_data.json'), 'w') as f:\n",
    "    json.dump(benchmark_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa01e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
